{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd22f84a-a20b-44ef-be0a-295579d69841",
   "metadata": {},
   "source": [
    "# Basic Neural Net from scratch\n",
    "\n",
    "This notebook has the purpose of creating a simple neural net from scratch only wiht pytorch. \n",
    "\n",
    "To do this, I need: \n",
    " - dependent and independent variable\n",
    " - weights & bias initialised\n",
    " - linear layer e.g. ReLu\n",
    " - activation function: e.g. sigmoid\n",
    " - loss function: e.g. mse\n",
    " - backwards, i.e. the parameters, gradients\n",
    "\n",
    "\n",
    "before doing the model - do not forget to clean the data and transform it so that most of the variation is between 0 and 1.\n",
    "\n",
    "With e.g. divide it by its maximum, so that it becomes like a percentage or use a sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f1a17-650c-4ccf-a580-4db90d945680",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## import data and shift it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa65df41-096e-4377-a1b8-49d364666c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11713, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_s3</th>\n",
       "      <th>open_s3</th>\n",
       "      <th>high_s3</th>\n",
       "      <th>low_s3</th>\n",
       "      <th>close_s3</th>\n",
       "      <th>vol_s3</th>\n",
       "      <th>time_s2</th>\n",
       "      <th>open_s2</th>\n",
       "      <th>high_s2</th>\n",
       "      <th>low_s2</th>\n",
       "      <th>...</th>\n",
       "      <th>high_s1</th>\n",
       "      <th>low_s1</th>\n",
       "      <th>close_s1</th>\n",
       "      <th>vol_s1</th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023.06.01 00:00</td>\n",
       "      <td>27103.1</td>\n",
       "      <td>27108.1</td>\n",
       "      <td>27080.6</td>\n",
       "      <td>27096.9</td>\n",
       "      <td>386.675</td>\n",
       "      <td>2023.06.01 00:15</td>\n",
       "      <td>27096.9</td>\n",
       "      <td>27096.9</td>\n",
       "      <td>27036.7</td>\n",
       "      <td>...</td>\n",
       "      <td>27077.4</td>\n",
       "      <td>27041.0</td>\n",
       "      <td>27054.9</td>\n",
       "      <td>275.080</td>\n",
       "      <td>2023.06.01 00:45</td>\n",
       "      <td>27054.9</td>\n",
       "      <td>27084.0</td>\n",
       "      <td>27054.8</td>\n",
       "      <td>27084.0</td>\n",
       "      <td>218.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023.06.01 00:15</td>\n",
       "      <td>27096.9</td>\n",
       "      <td>27096.9</td>\n",
       "      <td>27036.7</td>\n",
       "      <td>27047.0</td>\n",
       "      <td>408.680</td>\n",
       "      <td>2023.06.01 00:30</td>\n",
       "      <td>27047.0</td>\n",
       "      <td>27077.4</td>\n",
       "      <td>27041.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27084.0</td>\n",
       "      <td>27054.8</td>\n",
       "      <td>27084.0</td>\n",
       "      <td>218.143</td>\n",
       "      <td>2023.06.01 01:00</td>\n",
       "      <td>27084.0</td>\n",
       "      <td>27113.9</td>\n",
       "      <td>27073.5</td>\n",
       "      <td>27100.0</td>\n",
       "      <td>329.412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023.06.01 00:30</td>\n",
       "      <td>27047.0</td>\n",
       "      <td>27077.4</td>\n",
       "      <td>27041.0</td>\n",
       "      <td>27054.9</td>\n",
       "      <td>275.080</td>\n",
       "      <td>2023.06.01 00:45</td>\n",
       "      <td>27054.9</td>\n",
       "      <td>27084.0</td>\n",
       "      <td>27054.8</td>\n",
       "      <td>...</td>\n",
       "      <td>27113.9</td>\n",
       "      <td>27073.5</td>\n",
       "      <td>27100.0</td>\n",
       "      <td>329.412</td>\n",
       "      <td>2023.06.01 01:15</td>\n",
       "      <td>27100.0</td>\n",
       "      <td>27159.0</td>\n",
       "      <td>27100.0</td>\n",
       "      <td>27142.4</td>\n",
       "      <td>979.655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            time_s3  open_s3  high_s3   low_s3  close_s3   vol_s3  \\\n",
       "0  2023.06.01 00:00  27103.1  27108.1  27080.6   27096.9  386.675   \n",
       "1  2023.06.01 00:15  27096.9  27096.9  27036.7   27047.0  408.680   \n",
       "2  2023.06.01 00:30  27047.0  27077.4  27041.0   27054.9  275.080   \n",
       "\n",
       "            time_s2  open_s2  high_s2   low_s2  ...  high_s1   low_s1  \\\n",
       "0  2023.06.01 00:15  27096.9  27096.9  27036.7  ...  27077.4  27041.0   \n",
       "1  2023.06.01 00:30  27047.0  27077.4  27041.0  ...  27084.0  27054.8   \n",
       "2  2023.06.01 00:45  27054.9  27084.0  27054.8  ...  27113.9  27073.5   \n",
       "\n",
       "  close_s1   vol_s1              time     open     high      low    close  \\\n",
       "0  27054.9  275.080  2023.06.01 00:45  27054.9  27084.0  27054.8  27084.0   \n",
       "1  27084.0  218.143  2023.06.01 01:00  27084.0  27113.9  27073.5  27100.0   \n",
       "2  27100.0  329.412  2023.06.01 01:15  27100.0  27159.0  27100.0  27142.4   \n",
       "\n",
       "       vol  \n",
       "0  218.143  \n",
       "1  329.412  \n",
       "2  979.655  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "\n",
    "if iskaggle:\n",
    "    df = pd.read_csv(\"nnbasic-btc-data.csv\", index_col=0).reset_index(drop=True)\n",
    "else:\n",
    "    df = pd.read_csv(\"nnbasic-btc-data.csv\", index_col=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0efb72-f035-4dbb-aedc-074c5c4042ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not use the time columns \n",
    "\n",
    "df_train = df_merge.filter(items = ['open_s3', 'high_s3', 'low_s3', 'close_s3', 'vol_s3',\n",
    "       'open_s2', 'high_s2', 'low_s2', 'close_s2', 'vol_s2',\n",
    "       'open_s1', 'high_s1', 'low_s1', 'close_s1', 'vol_s1', \n",
    "       'open', 'high', 'low', 'close', 'vol']).dropna()\n",
    "print(df_train.shape)\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da202235-e6bb-4e45-8a68-f62191d17acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train\n",
    "#df_train.filter(items = [\"open_s3\"])\n",
    "#df_train.loc[[\"open_s3\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f80f7-7005-4c31-9ca1-4bb869c8ab82",
   "metadata": {},
   "source": [
    "# fastai neural net from scratch\n",
    "\n",
    "https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dbbac0-4b96-4eda-8eb2-6717c2d3bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211d88a-6139-4a62-856e-62d55b2421fa",
   "metadata": {},
   "source": [
    "from https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "\"PyTorch provides methods to create random or zero-filled tensors, which we will use to create our weights and bias for a simple linear model. These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. **This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!**\n",
    "\n",
    "For the weights, we set requires_grad after the initialization, since we don’t want that step included in the gradient. (Note that a trailing _ in PyTorch signifies that the operation is performed in-place.)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29835d73-af68-4ca8-b034-f422079fbed2",
   "metadata": {},
   "source": [
    "## dep, indep & weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698cabc-d81f-488b-9b08-0da27f972a43",
   "metadata": {},
   "source": [
    "In this setting we will just try to predict the high value based on the previous three candles. The results will likely not be good since point predictions are tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d57916-3ed2-410d-8188-8b548d2e15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dep = tensor(df_train['high'].values, dtype=torch.float)\n",
    "print(t_dep.shape) # 1 D tensor\n",
    "t_dep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0297673-ad3b-44eb-8cf0-298ee966f5d0",
   "metadata": {},
   "source": [
    "All variables are used instead of the high value which is the dependend value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb70a0-a12d-40f8-b6bc-d2a59badc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_indep = tensor(df_train.loc[:, df_train.columns != \"high\"].values, dtype=torch.float)\n",
    "\n",
    "t_indep.shape # 2 D tensor > rows with observations and columns with features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d97af7f-e8a3-4116-a8fe-b00eb218cf5f",
   "metadata": {},
   "source": [
    "The number of features needs to be the same as the lenght of the weights, because they will be multiplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6b332-1450-4dc1-bafb-036358841e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "n,c = t_indep.shape\n",
    "\n",
    "# \"We are initializing the weights here with Xavier initialisation (by multiplying with 1/sqrt(n)).\"\n",
    "weights = torch.randn(c) / math.sqrt(n)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(n, requires_grad=True)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ac892-c9b8-4286-a122-d1fd36b91dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_indep*weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da184691-85e6-410a-a142-e7a85716df6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sidebar: torch.randn\n",
    "short check of the torch.randn function: https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e73f77-a591-480c-9a46-e05b4460ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(19, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340f77f-8119-496d-94cf-7a43f7321768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coefficients = weights\n",
    "# make them from -0.5 to 0.5 from a normal distribution\n",
    "coeffs = torch.rand( t_indep.shape[1] )-0.5\n",
    "\n",
    "print(coeffs.shape)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49848d-809b-480c-8748-25a62f5bd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "(t_indep*coeffs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9fa5f-6dda-4e60-961a-296953ce85dc",
   "metadata": {},
   "source": [
    "As you can see, there are a lot of numbers up and down, since the prices range roughly from 30k to about 60k. \n",
    "\n",
    "Note to myself: if it does not interfere with cross entropy then I should use log on all prices.\n",
    "cross entropy uses log in its calculations and it depends on the implementation if it does interfere (I guess right now)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a048cbf-1602-4bbb-a19b-b2b85dfcc53b",
   "metadata": {},
   "source": [
    "## making the first prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb0985-8922-4952-b2f7-59ba326b03c9",
   "metadata": {},
   "source": [
    "Since we are making a linear classifier the formula for yhat or the predictions is the sum of the products of coefficients and observations. \n",
    "\n",
    "In other words: \n",
    "\n",
    "$ y = x_0 \\times b_0 + x_1 \\times b_1 $\n",
    "\n",
    "where $x_0$ is the first vector of observations (=features) and $b_0$ is the first coefficient.\n",
    "\n",
    "and then is $y$ the vector with the predictions the same length as the data frame, so this formula can be read \"rowwise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a7453-6dd6-490d-b01b-04de3157fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (t_indep*coeffs).sum(axis=1)\n",
    "print(preds.shape) # 1d vector of predictions\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e8bee-542c-4f10-b482-6b1f76f39443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check for first row: \n",
    "\n",
    "print(t_indep[0])\n",
    "print(coeffs)\n",
    "print(\"\\n first entry of Indep * coeffs: \\n\",t_indep[0][0]*coeffs[0])\n",
    "print(\"\\n Indep * coeffs: \\n\",t_indep[0]*coeffs)\n",
    "print(\"\\n sum of Indep * coeffs: \\n\", (t_indep[0]*coeffs).sum() ) #axis=1 is along the colums\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20783ec4-6f02-445a-8956-9f8b147e9c9e",
   "metadata": {},
   "source": [
    "## loss: cross entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3192526-bcce-41f6-a995-2eef783b648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "loss_ce = F.cross_entropy(preds, t_dep)\n",
    "loss_mae = F.l1_loss(preds, t_dep)\n",
    "loss_mae = F.mse_loss(preds, t_dep)\n",
    "\n",
    "print(\"mean absolute error:\",loss_mae)\n",
    "print(\"mean squared error:\",loss_mae)\n",
    "print(\"cross entropy:\",loss_ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b03dbf-97f7-42c3-9dbe-9e3cff4afe2b",
   "metadata": {},
   "source": [
    "## gradients & learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c54b9-b9f5-4890-a5b9-41138163c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd3fc5-81ee-41ea-b66c-0e19384d709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d627d8b-c09e-45ab-b645-5c64f0d1aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am not sure if the above way works if I only use the loss functions in a functional style\n",
    "# therefore, I document a way to use it instantiated as a class\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "cel = nn.CrossEntropyLoss()\n",
    "\n",
    "preds = (t_indep*coeffs.requires_grad_()).sum(axis=1)\n",
    "\n",
    "loss = cel(preds,t_dep)\n",
    "print(loss)\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "gradients = coeffs.grad\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6e4a4-d0e2-40ab-9698-f9d0283465e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(retain_graph=True)\n",
    "print(coeffs.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161a5a0-9d80-4d17-b1fc-060b09f326df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise coeffs and define the independent variable \n",
    "t_indep*coeffs\n",
    "\n",
    "# define it as a function \n",
    "def calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\n",
    "    \n",
    "preds = (t_indep*coeffs).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0bbb9-8ac5-45e2-b72c-a0dd6cd3a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "\n",
    "# herer it is mean absolute error\n",
    "loss = torch.abs(preds-t_dep).mean()\n",
    "\n",
    "# define it in a function\n",
    "def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692b898-df59-48dd-9f98-46e16ccbc215",
   "metadata": {},
   "source": [
    "## next steps\n",
    "\n",
    " - figure out when and how to use backward() and require_grad\n",
    " - put the pieces togehter: calc preds > calc loss > get gradients > subtract gradients*learningrate with preds\n",
    "\n",
    "Note on gradients: \\n\n",
    "My current understand is that you need to specify the tensor of where to calculate the gradients on, to access them explicitly. \\n\n",
    "It seems, that the backward function needs to know the input, target and weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7108317-bba0-4744-a215-1f64cf5e03a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# pytorch beginner tutorial by jeremy howard\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nn_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec8e12-a769-4f3d-a40e-858956eed9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "xb = torch.randn(64, 784)\n",
    "weights = torch.randn(784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587893d9-e412-457e-a254-41a0645a94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xb@weights\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd206a9-096d-4a64-af23-032111d4e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xb.shape)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e45088-2395-46fd-8ac8-aa5496cbc093",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# pytorch basics tutorial\n",
    "see: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd45d48-e909-43ef-b3ef-0e4464924c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# define the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039b205-b28d-46b2-bb8a-9360178928ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db1850-dd89-433d-8000-42376567d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048d1e2-26a7-40c5-b246-df7f2a5985cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3b0a2-8344-4ecb-b23d-0abec67ccbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f46f7e-9967-4add-a5e6-e4609f377c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc09c76-d517-46e5-8d4d-d96f9e139afe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sidebar: nn.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f92d0-4ac2-4f53-b93b-f7f6944d551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.ones(3,2,2)\n",
    "print(input_image.size())\n",
    "\n",
    "input_image[1].add_(1)\n",
    "input_image[2].add_(2)\n",
    "input_image[0,1,1].add_(1)\n",
    "\n",
    "input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf2787-9515-432f-a815-7334057284f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "flat_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a51b00-0572-4c53-8b68-644f22f15981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer1 = nn.Linear(in_features=2*2, out_features=2,bias=False)\n",
    "import torch.nn.init as init\n",
    "init.ones_(layer1.weight)\n",
    "\n",
    "linear_layer = layer1(flat_image)\n",
    "\n",
    "print(f\"\\nlinear layer shape:{linear_layer.size()}\" )\n",
    "w = layer1.weight # has the in_features shape, meaning it is a vector of length 4\n",
    "print(f\"\\nweight shape:{w.shape} = [out_features,in_features]\")\n",
    "print(\"\\nlinear layer weights:\\n\",w)\n",
    "#print(\"\\noutput divided by input\\n\",linear_layer/flat_image)\n",
    "print(f\"\\ny=output = \\n{linear_layer}\")\n",
    "# output should be: \n",
    "flat_image@w.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b88ba2-e368-4e25-9184-d43f9ddd8a70",
   "metadata": {},
   "source": [
    "There for the transformation of $ y = xW^T+b $ where x=in_features, y=out_features, W=weights and b=bias\n",
    "\n",
    " - nn.Linear generates a random weight matrix with shape of [out_features,in_features]\n",
    " - nn.Linear adds all the columns together\n",
    " - the added rows are multiplied with the weight matrix\n",
    " - optionally a bias term is added\n",
    "\n",
    "By summing all input columns a matrix with length of in_features is present to be multiplied with the correct dimension of the weight matrix.\n",
    "\n",
    "see: https://ashwinhprasad.medium.com/pytorch-for-deep-learning-nn-linear-and-nn-relu-explained-77f3e1007dbb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a9b40-bdfb-4a0b-a68d-bd4379428a04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## sidebar: tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be4b72-2fec-47b3-ae7f-2e0e7d408829",
   "metadata": {},
   "source": [
    "One can think of tensors in terms of the below element has an element of 3 which contains 2 elements which contain 4 elements which contain 2 elements, hence resultig in a nested list of two.\n",
    "\n",
    "see also: \n",
    " - https://stackoverflow.com/questions/52370008/understanding-pytorch-tensor-shape\n",
    " - https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9462c-58fc-41e7-b027-899cad9b8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn(3,2,4,2)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5927880-f03e-4907-9732-061ae6ceeffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first layer of the tensor consists of three elements of which the third is printed\n",
    "# this third element of the first layer has 2 elements which contain 4 elements which in turn contain 2\n",
    "# the last two form then the vector as printed in the brackets\n",
    "test[2] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc41621-175e-4ef4-8116-6d0ecdb2bb6e",
   "metadata": {},
   "source": [
    "## next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
